{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66892f2a-1c92-47d6-a9c1-8cb26940f251",
   "metadata": {},
   "source": [
    "### Tutorial 2 - How to extend DASF Datasets\n",
    "\n",
    "In this tutorial, we show how you can extend DASF datasets to be loaded dynamically inpendent of the host architecture.\n",
    "\n",
    "For this specific scenario we will use DASF Array Dataset class to show you how you can create a dataset like this using a simple NPY file.\n",
    "An array dataset is a dataset stored in numpy-like format. It is a simple file that contains a numpy array serialized in a file. Note that, if you are using GPU, the data will be loaded as cupy array, else it will be loaded as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c38bd978-012f-4599-a249-b20577e3700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/cupy/_environment.py:447: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "\n",
    "# Dasf imports\n",
    "from dasf.datasets import DatasetArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffa541",
   "metadata": {},
   "source": [
    "To start, the first step is create an numpy array and save it to a numpy file (`.npy`). Therefore, we will create a simple cube with random data, with shape (20, 20, 20) and save it to a file called `data.npy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9f1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we generate a random array and save it in a numpy-like file\n",
    "data = np.random.random((20, 20, 20))\n",
    "np.save(\"data.npy\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b5760-bbca-4613-9958-bbaacd96ced6",
   "metadata": {},
   "source": [
    "Once we have the file saved in a numpy-like file format, we can load it using `DatasetArray` class. This class is a simple class that loads a numpy file and returns a numpy array. Note that, if you are using GPU, the data will be loaded as cupy array, else it will be loaded as numpy array.\n",
    "\n",
    "The `name` parameter is optional and specifies the symbolic dataset name. This name will be used to identify the dataset in the DASF framework. The `root` parameter specifies the location of the file to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7774cf93-a8ff-46d6-b378-0a5eff657b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset: name=My Saved NPY, root=, loaded=False, data shape=None"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetArray(name=\"My Saved NPY\", root=\"data.npy\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f92ca2-ecb6-4241-b2be-726146056259",
   "metadata": {},
   "source": [
    "Dasf datasets are lazy, which means that it is not loaded immediatly. The `load` function will load the dataset and return the data. Note that, if you are using GPU, the data will be loaded as cupy array, else it will be loaded as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b055d3f-0d96-4f1e-bcde-20bd825976bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n",
      "Dataset: name=My Saved NPY, root=, loaded=False, data=None <function DatasetArray.load at 0x7f5a77cb5b40> True False _load_gpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset: name=My Saved NPY, root=, loaded=True, data=(20, 20, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd783da-95e1-42cf-8ab9-da5035641a05",
   "metadata": {},
   "source": [
    "Once it is loaded, we can slice the dataset and see the type of the data. If you have gpus in your machine, you can see that the data is a cupy array else it is a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25dbe78-7abf-4ef2-a564-ed72a0bc79e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cupy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733a245-13f1-45b3-8b69-0d09a496c61d",
   "metadata": {},
   "source": [
    "What should I do if I'm using a GPU but I want to load a Numpy array?\n",
    "\n",
    "All the datasets have a protected load wrapper for each platform. The code discovers which platform you are in and bind the method to its respective protected mathod.\n",
    "\n",
    "In other words, if you are using `load` in a GPU environment as we are doing here, in fact you are executing the protected method called `_load_gpu`.\n",
    "\n",
    "Then to load Numpy arrays, all you need to do is call directly `_load_cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab10622-978a-4e2e-af2c-6cd9c4f6c626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._load_cpu()\n",
    "\n",
    "type(dataset[:2, :2, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a38825-1bd7-42f0-8d65-b09553e273b4",
   "metadata": {},
   "source": [
    "If you need to handle a Dask array in a multi clustered environment, you can use the protected lazy methods called `_lazy_*`.\n",
    "\n",
    "For datasets, the respective methods for `load` are `_lazy_load_cpu` and `_lazy_load_gpu`. Both returns a Dask Array but with different metadata.\n",
    "\n",
    "Let's see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ec3104-8087-406c-aa86-71961740fcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array.core.Array"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._lazy_load_cpu()\n",
    "\n",
    "type(dataset[:2, :2, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc2c21-ce64-42a3-a72a-b9f895decdb9",
   "metadata": {},
   "source": [
    "See how the internal array of this Dask dataset looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a556a1-643d-449e-8f5b-b5223972dbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[:2, :2, :2]._meta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "test_requirements": {
   "single_gpu": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
